# üåê Sistema WebScrapping

## üóíÔ∏è Colaboradores

```
‚îú‚îÄ‚îÄ Santiago Gamboa Mart√≠nez
‚îú‚îÄ‚îÄ Samuel Eduardo Fajardo Quintero
‚îú‚îÄ‚îÄ Alejandro Baca Torregroza
‚îú‚îÄ‚îÄ Alessandro Garzon Melo
‚îú‚îÄ‚îÄ Omar Daniel Calvache Madro√±ero
‚îî‚îÄ‚îÄ Nicolas David Lovera Cabiativa
```

# üèÜ Introducci√≥n

El volumen de informaci√≥n que se encuentra disponible en internet crece de manera exponencial, haciendo indispensable el uso de herramientas tecnol√≥gicas que permitan extraer y analizar datos relevantes de forma autom√°tica y eficiente. Por esta raz√≥n, como equipo, hemos elegido desarrollar la decidido crear un **_Sistema de WebScrapping_**, este proyecto consiste en desarrollar e implementar un sistema de web scraping que no solo cumpla con los objetivos de extracci√≥n de datos, sino que tambi√©n est√© dise√±ado aplicando principios fundamentales de la Programaci√≥n Orientada a Objetos (POO) y optimizado bajo los principios del manejo de estructuras de datos.

El objetivo principal es garantizar una gesti√≥n eficiente de la informaci√≥n mediante la integraci√≥n de dos componentes: Python y Java. Python se encargar√° de la navegaci√≥n y extracci√≥n de datos desde plataformas de e-commerce, mientras que Java funcionar√° como el n√∫cleo de procesamiento, enlazando los datos obtenidos con la interfaz gr√°fica (GUI). Para lograr un manejo y an√°lisis r√°pidos, Java organizar√° la informaci√≥n en estructuras de datos en memoria ‚Äîcomo arreglos din√°micos, √°rboles binarios y heaps‚Äî aprovechando las fortalezas de cada una para optimizar el rendimiento y la funcionalidad del sistema.

### Ventajas de este enfoque, centrado en ED:

- Eficiencia en Memoria: Java gestiona los objetos *Productos* en memoria, permitiendo manipulaciones r√°pidas sin depender de constantes lecturas al disco.
- Organizaci√≥n L√≥gica: Los datos no son simples cadenas de texto; se estructuran en colecciones dinamicas o lo mismo *ArrayList*; preparadas para ser integradas con estructuras m√°s complejas como BST o Heaps para ordenamiento por precio o calificaci√≥n.
- Interoperabilidad: Uso de *ProccesBuilder* y flujos de entrada/salida para comunicar dos entornos de programaci√≥n distintos.


# üóÇÔ∏è Requerimientos T√©cnicos

## 1. Lenguaje y Librer√≠as

- Python se va a encargar de ser el motor de scraping subyacente.
- Java ser√° el lenguaje principar para la logica de negocio y estructuras de datos.

### Librer√≠as Utilizadas

A continuaci√≥n, se listan las librer√≠as clave utilizadas en este proyecto, junto con una breve descripci√≥n de su funcionalidad:

```
PYTHON
Flask                # Desarrollo de aplicaciones web y creaci√≥n de APIs
SQLAlchemy           # Manejo de bases de datos ORM para facilitar la interacci√≥n con datos estructurados
requests            # Realizar solicitudes HTTP de manera sencilla y eficiente
beautifulsoup4      # Parsear y extraer datos de estructuras HTML y XML
selenium            # Automatizaci√≥n e interacci√≥n con p√°ginas web din√°micas
python-dotenv       # Gesti√≥n de variables de entorno para mayor seguridad y flexibilidad
colorama            # Mejorar la visualizaci√≥n de mensajes en la terminal con colores

JAVA
util.ArrayList      # Estructura base para el historial de productos
lang.ProcessBuilder # Para la orquestaci√≥n y ejecuci√≥n del script de Python
util.regex          # Para el parseo y limpieza de la informaci√≥n entrante (JSON strings)
```

## 2. Estructura del C√≥digo

Para garantizar escalabilidad y facilidad de mantenimiento, el c√≥digo sigue los principios de **Programaci√≥n Orientada a Objetos (POO)**. Esto permite la reutilizaci√≥n de componentes y una mejor organizaci√≥n del sistema.

### Clases principales

#### PYTHON
- **```WebDataExtractor```**: Clase base que define la estructura general del proceso de extracci√≥n de datos.
- **```StaticPageExtractor```**: Extiende `WebDataExtractor` para manejar p√°ginas web est√°ticas.
- **```DynamicPageExtractor```**: Extiende `WebDataExtractor` para manejar p√°ginas web din√°micas con `Selenium`.
- **```DataHandler```**: Responsable del almacenamiento y procesamiento de los datos extra√≠dos.
- **```ScrapingCoordinator```**: Coordina la ejecuci√≥n del proceso de scraping y gestiona las diferentes clases.
  
Por otra parte, para garantizar la gesti√≥n eficiente de estructuras de datos y en la conexi√≥n directa con la interfaz gr√°fica. Java act√∫a como el n√∫cleo de procesamiento del sistema: recibe los datos generados por los m√≥dulos de Python, los transforma y los organiza en estructuras internas optimizadas.

### Archivos principales

#### JAVA
- **```App.Java```**: Main class, dirige las llamadas al scraper y muestra estad√≠sticas acumuladas.
- **```DataManager.java```**: El puente, ejecuta el proceso de Python, captura el jlujo de datos (stdout), limpia los JSON strings y puebla las estructuras de datos en Java
- **```Producto.java```**: El nodo de informaci√≥n, representa el objeto con atributos normalizados
- **```RunPython.java```**: Encapsula la complejidad de invocar el int√©rprete de Python y gestionar los argumentos de entrada/salida

Cada uno de estos componentes est√° dise√±ado para manejar su propia funcionalidad: los m√≥dulos en Python se enfocan en la extracci√≥n y preparaci√≥n de datos, mientras que los m√≥dulos en Java gestionan las estructuras internas y la interacci√≥n con la interfaz gr√°fica. Esta separaci√≥n clara de responsabilidades reduce la dependencia entre m√≥dulos y facilita la escalabilidad y la extensi√≥n del sistema en el futuro.

## 3. Entorno de Desarrollo

Para asegurar una experiencia de desarrollo eficiente y organizada, se establecen las siguientes pr√°cticas:

- **Uso de entornos virtuales (`venv`)**: Permite aislar dependencias y evitar conflictos con otras instalaciones de Python.
- **Control de versiones con `Git`**: Se emplea `Git` para rastrear cambios en el c√≥digo, facilitar la colaboraci√≥n y garantizar la estabilidad del proyecto.
- **Definici√≥n de dependencias en `requirements.txt`**: Se listan todas las librer√≠as requeridas para que el entorno pueda ser replicado f√°cilmente en diferentes sistemas.
- **Uso de archivos de configuraci√≥n (`.env`)**: Permite almacenar credenciales y configuraciones sensibles sin exponerlas en el c√≥digo fuente.

## 4. Flujo de Datos y Estructuras

1. **Solicitud**: Java solicita datos.
2. **Extracci√≥n**: Python navega y extrae.
3. **Transmisi√≥n**: Los datos viajan v√≠a stdout en formato string/JSON.
4. **Estructuraci√≥n**: Java recibe los bytes, reconstruye los objetos **Producto** y los inserta en un **Historial Global** (`ArrayList`).
5. **Manipulaci√≥n**: Los datos en la estructura permiten:
   - Filtrado por tienda.
   - Conversi√≥n de precios para futuros ordenamientos.
   - Generaci√≥n de reportes acumulativos.

Los datos extra√≠dos pueden ser almacenados en m√∫ltiples formatos seg√∫n las necesidades del proyecto:

- **CSV**: Para manipulaci√≥n en hojas de c√°lculo.
- **JSON**: Para intercambio de datos estructurados.
- **SQLite**: Para almacenamiento en bases de datos locales y consultas estructuradas.

El sistema est√° dise√±ado para adaptarse a diferentes formatos sin modificar la l√≥gica central del scraping.

### Estructuras de datos utilizadas en Java

Adem√°s del flujo anterior, la capa de Java organiza los objetos **Producto** en varias estructuras de datos para permitir diferentes tipos de consultas:

- **`ArrayList<Producto>` ‚Äì Historial global**  
  Es la primera estructura que se llena con los productos provenientes de Python.  
  Desde este historial:
  - Se reconstruyen las dem√°s estructuras al iniciar el programa.
  - Se calculan estad√≠sticas generales (cantidad total de productos, conteo por tienda, etc.).
  - Se muestran listados completos en la interfaz de consola.

- **√Årbol AVL ‚Äì B√∫squeda por nombre de producto**  
  Los productos se insertan en un √°rbol AVL utilizando como clave el **t√≠tulo normalizado** (sin tildes y en min√∫sculas).  
  Esto permite:
  - Buscar productos por t√©rmino de texto en tiempo cercano a `O(log n)`.
  - Recorrer el √°rbol en orden alfab√©tico para mostrar los nombres ordenados.
  - Obtener subconjuntos de productos que coinciden con una palabra clave para luego analizarlos con otras estructuras (Heap o BST).

- **Heap m√≠nimo ‚Äì Top N productos m√°s baratos**  
  A partir del historial o de un subconjunto filtrado, se construye un *heap* m√≠nimo utilizando como clave el **precio num√©rico** del producto.  
  Con esta estructura se puede:
  - Obtener el **top N productos m√°s baratos** sin ordenar toda la lista.
  - Comparar la eficiencia del Heap frente a ordenar el arreglo completo cada vez que se hace una consulta.

- **√Årbol binario de b√∫squeda (BST) ‚Äì B√∫squeda por rango de precios**  
  Tambi√©n se inserta cada producto en un BST, donde la clave es nuevamente el precio.  
  El BST se utiliza para responder consultas como:
  - ‚ÄúMostrar todos los productos con precio entre A y B‚Äù.
  - Recorrer solo los nodos cuyo precio est√° en el rango `[precioMin, precioMax]`, evitando revisar todos los elementos del historial.  
  Esto hace que las **consultas por rango** sean m√°s eficientes y mejor estructuradas.

Gracias a la combinaci√≥n de estas estructuras, el sistema no solo almacena los datos extra√≠dos, sino que tambi√©n permite realizar operaciones de b√∫squeda, filtrado y an√°lisis de forma eficiente, que es uno de los objetivos principales del proyecto en el contexto de Estructuras de Datos.

## 5. Otros Requerimientos y Consideraciones

- **Manejo de excepciones**: Se implementan mecanismos de captura de errores para evitar interrupciones inesperadas en la ejecuci√≥n del programa.
- **Compatibilidad con m√∫ltiples tipos de sitios web**: El sistema est√° dise√±ado para funcionar con p√°ginas est√°ticas y din√°micas.
- **Optimizaci√≥n del rendimiento**: Se eval√∫an estrategias como el uso de `asyncio` y `multithreading` para mejorar la eficiencia en el scraping.
- **Escalabilidad**: La arquitectura modular permite agregar nuevas fuentes de datos o expandir funcionalidades sin afectar la estructura existente.

Este conjunto de pr√°cticas y herramientas asegura un flujo de trabajo robusto y adaptable a diferentes necesidades del proyecto.

# üõ†Ô∏è Configuraci√≥n del Entorno de Trabajo

### **1. Clonar el repositorio**:

Descargar el c√≥digo fuente con los siguientes comandos:

```bash
git clone https://github.com/santgm56/SCDataExtractor_Project.git
cd Super-Proyecto-Final
```

### **2. Crear y activar un entorno virtual**:

El uso de un entorno virtual ayuda a instalar las dependencias del proyecto sin interferir con otras aplicaciones de Python.

**En Windows**:

```bash
python -m venv venv
.\venv\Scripts\activate
```

**En macOS/Linux**:

```bash
python -m venv venv
source venv/bin/activate
```

### **3. Instalar las dependencias**:

Una vez dentro del entorno virtual, ejecutar:

```bash
pip install -r requirements.txt
```
### **4. Compilar y Ejecutar Java:**

El punto de entrada es ahora Java. Aseg√∫rarse de estar en la ra√≠z del proyecto es importante.

```bash
javac -d bin SCDataExtractor_Project/App/src/*.java

# Ejecutar la aplicaci√≥n
java -cp bin App
```

### **5. Salir del entorno virtual**:

Al terminar de trabajar o hacer modificaciones, se puede salir del entorno virtual escribiendo:

```bash
deativate
```

Si se usa Windows y existe alg√∫n problema al activar el entorno virtual, es posible que se necesite habilitar la ejecuci√≥n de scripts por pol√≠ticas de resticci√≥n en powershell. Para corregirlo, basta con ejecutar estos comandos en el CMD como terminal predeterminada ya que esta no cuenta con dichas condiciones.


# ‚ú® Estructura del proyecto

```plaintext
SUPER_PROYECTO_FINAL/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ Estructura_Proyecto.txt
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.py
‚îú‚îÄ‚îÄ App/                             # M√≥dulo Java (estructuras de datos)
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ compile.ps1                  # Script para compilar en Windows
‚îÇ   ‚îú‚îÄ‚îÄ run.ps1                      # Script para ejecutar en Windows
‚îÇ   ‚îú‚îÄ‚îÄ setup.ps1                    # Configuraci√≥n inicial del m√≥dulo Java
‚îÇ   ‚îú‚îÄ‚îÄ .vscode/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.json            # Configuraci√≥n del editor
‚îÇ   ‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ARQUITECTURA.md          # Detalles de arquitectura del m√≥dulo Java
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DESARROLLO.md            # Notas de desarrollo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ INSTALACION.md           # Gu√≠a de instalaci√≥n (Java)
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ App.java                 # Main Java ‚Äì interfaz de consola
‚îÇ       ‚îú‚îÄ‚îÄ AVLTree.java             # Implementaci√≥n del √Årbol AVL
‚îÇ       ‚îú‚îÄ‚îÄ BST.java                 # √Årbol binario de b√∫squeda (rangos de precio)
‚îÇ       ‚îú‚îÄ‚îÄ DataManager.java         # Gestor del historial y de las ED
‚îÇ       ‚îú‚îÄ‚îÄ Heap.java                # Heap m√≠nimo (top N productos m√°s baratos)
‚îÇ       ‚îú‚îÄ‚îÄ HistorialDB.java         # Manejo de la base de datos de historial
‚îÇ       ‚îú‚îÄ‚îÄ Producto.java            # Modelo de producto
‚îÇ       ‚îî‚îÄ‚îÄ RunPython.java           # Ejecutor de Python via ProcessBuilder
‚îú‚îÄ‚îÄ src/                             # M√≥dulo Python (scraping + API + BD)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Configuraci√≥n general y selectores
‚îÇ   ‚îú‚îÄ‚îÄ base/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ web_data_extractor.py    # Clase base WebDataExtractor
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_handler.py          # Almacenamiento en JSON/SQLite y reportes
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dynamic/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ dynamic_page_extractor.py # Base para scrapers din√°micos (Selenium)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ecommerce_extractor.py    # Scraper de e-commerce
‚îÇ   ‚îÇ  
‚îÇ   ‚îú‚îÄ‚îÄ coordinator/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraping_coordinator.py  # Coordinador de tareas de scraping
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py              # Conexi√≥n a SQLite mediante SQLAlchemy
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py                # Definici√≥n de modelos ORM
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ helpers.py               # Funciones auxiliares (validaci√≥n, paths, etc.)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py                # Configuraci√≥n de logging
‚îÇ   ‚îî‚îÄ‚îÄ web/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ app.py                   # Creaci√≥n de la app Flask
‚îÇ       ‚îî‚îÄ‚îÄ routes.py                # Rutas de la API REST
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îú‚îÄ‚îÄ .gitkeep                     # Placeholder para el directorio
‚îÇ   ‚îî‚îÄ‚îÄ scraping.log                 # Registro de ejecuci√≥n del scraping
‚îî‚îÄ‚îÄ outputs/
    ‚îî‚îÄ‚îÄ scraped_data.db              # Base de datos SQLite con los datos extra√≠dos
```

El proyecto est√° organizado de manera modular y jer√°rquica, siguiendo buenas pr√°cticas de desarrollo de software. A continuaci√≥n, se explica cada componente y su importancia, as√≠ como las ventajas de utilizar esta estructura:

### **1. Archivos Ra√≠z**
- **`README.md`**:  
  - **Importancia**: Es la primera impresi√≥n del proyecto. Proporciona una descripci√≥n general, instrucciones de instalaci√≥n, uso y documentaci√≥n clave.  
  - **Ventajas**: Facilita la comprensi√≥n del proyecto para nuevos desarrolladores o colaboradores. Es esencial para proyectos open-source o colaborativos.  

- **`requirements.txt`**:  
  - **Importancia**: Lista todas las dependencias necesarias para ejecutar el proyecto.  
  - **Ventajas**: Permite replicar el entorno de desarrollo f√°cilmente con `pip install -r requirements.txt`. Asegura que todos los colaboradores usen las mismas versiones de las librer√≠as.  

- **`.gitignore`**:  
  - **Importancia**: Especifica archivos y directorios que no deben ser rastreados por Git (por ejemplo, `__pycache__`, `logs/`, `outputs/`).  
  - **Ventajas**: Evita la inclusi√≥n de archivos innecesarios en el repositorio, como archivos temporales o datos sensibles.  

- **`setup.py`**:  
  - **Importancia**: Script para instalar el proyecto y sus dependencias. Puede incluir metadatos y configuraciones de instalaci√≥n.  
  - **Ventajas**: Facilita la distribuci√≥n e instalaci√≥n del proyecto como un paquete Python.  

- **`main.py`**:  
  - **Importancia**: Punto de entrada principal del scraper. Contiene la l√≥gica para iniciar el proceso de scraping.  
  - **Ventajas**: Centraliza la ejecuci√≥n del proyecto, lo que simplifica la interacci√≥n con el usuario final.  

### **2. Directorio `App/` (M√≥dulo Java - Estructuras de Datos)**
Este directorio contiene el n√∫cleo l√≥gico y de gesti√≥n de datos del proyecto, implementado en Java para aprovechar su tipado fuerte y eficiencia en memoria.

### 2. Directorio `App/` (M√≥dulo Java ‚Äì Estructuras de Datos)

Este directorio contiene el n√∫cleo l√≥gico y de gesti√≥n de datos del proyecto, implementado en Java para aprovechar su tipado fuerte y eficiencia en memoria.

#### 2.1. `src/` (C√≥digo fuente Java)

**App.java**  
- **Importancia**: Es el punto de entrada principal del sistema en Java. Orquesta la ejecuci√≥n, mostrando men√∫s, opciones de scraping y estad√≠sticas acumuladas.  
- **Ventajas**: Centraliza el flujo del programa y la interacci√≥n con el usuario desde un entorno robusto.

**DataManager.java**  
- **Importancia**: Act√∫a como el ‚Äúcerebro‚Äù de la gesti√≥n de datos. Parsea la salida de Python y puebla las estructuras de datos en memoria (historial y estructuras auxiliares).  
- **Ventajas**: Permite manipular, filtrar y acumular datos de m√∫ltiples b√∫squedas sin depender constantemente del acceso a disco.

**Producto.java**  
- **Importancia**: Define el modelo de datos (objeto/nodo) para cada producto. Normaliza atributos como el precio (de texto a num√©rico) y el t√≠tulo (tildes, may√∫sculas, etc.).  
- **Ventajas**: Garantiza la integridad de la informaci√≥n y facilita la implementaci√≥n de algoritmos de b√∫squeda y ordenamiento.

**RunPython.java**  
- **Importancia**: Gestiona la interoperabilidad entre Java y Python mediante `ProcessBuilder`, ejecutando `main.py` dentro del entorno virtual.  
- **Ventajas**: Encapsula la complejidad de ejecutar subprocesos, manejar stdin/stdout y capturar errores del script de scraping.

#### 2.2. Clases de estructuras de datos en Java

**AVLTree.java**  
- **Importancia**: Implementa un √Årbol AVL que mantiene los productos ordenados por un criterio (por ejemplo, t√≠tulo normalizado).  
- **Ventajas**: Permite b√∫squedas y recorridos en orden alfab√©tico en tiempo cercano a `O(log n)`.

**BST.java**  
- **Importancia**: Implementa un √Årbol Binario de B√∫squeda usando t√≠picamente el precio como clave.  
- **Ventajas**: Facilita consultas por rango de precios (por ejemplo, productos entre A y B) sin recorrer todos los elementos.

**Heap.java**  
- **Importancia**: Implementa un Heap m√≠nimo a partir del precio num√©rico del producto.  
- **Ventajas**: Permite obtener el ‚ÄúTop N productos m√°s baratos‚Äù de forma eficiente, sin ordenar toda la lista cada vez.

**HistorialDB.java**  
- **Importancia**: Gestiona el almacenamiento del historial de productos en una base de datos (lado Java).  
- **Ventajas**: Permite persistir la informaci√≥n entre ejecuciones y reconstruir las estructuras de datos en memoria al iniciar el programa.

---

### **3. Directorio `src/` (C√≥digo Fuente Python)**  
Este directorio contiene el n√∫cleo del proyecto, organizado en m√≥dulos y subdirectorios espec√≠ficos.  

#### **3.1. `config.py`**  
- **Importancia**: Centraliza la configuraci√≥n del proyecto (por ejemplo, timeouts, selectores CSS, credenciales de API).  
- **Ventajas**: Facilita la modificaci√≥n de par√°metros sin necesidad de alterar el c√≥digo fuente. Mejora la mantenibilidad.  

#### **3.2. `base/`**  
- **Importancia**: Contiene la clase base abstracta `web_data_extractor.py`, que define la interfaz com√∫n para todos los extractores.  
- **Ventajas**: Promueve la reutilizaci√≥n de c√≥digo y asegura que todos los extractores sigan un patr√≥n com√∫n (herencia y polimorfismo).  

#### **3.3. `components/`**  
- **Importancia**: Contiene los m√≥dulos espec√≠ficos para el scraping, divididos en:  
  - **`static_page_extractor.py`**: Extracci√≥n de p√°ginas est√°ticas (HTML/CSS).  
  - **`dynamic/`**: Extracci√≥n de p√°ginas din√°micas (JavaScript), con m√≥dulos espec√≠ficos para e-commerce y bienes ra√≠ces.  
  - **`data_handler.py`**: Manejo de datos extra√≠dos (JSON, SQL).  
- **Ventajas**: La modularidad permite agregar nuevos tipos de extractores sin afectar el c√≥digo existente. Facilita las pruebas y el mantenimiento.  

#### **3.4. `coordinator/`**  
- **Importancia**: Contiene `scraping_coordinator.py`, que gestiona el flujo de trabajo del scraping (descarga, extracci√≥n y almacenamiento).  
- **Ventajas**: Centraliza la l√≥gica de coordinaci√≥n, lo que simplifica la ejecuci√≥n de tareas complejas y mejora la escalabilidad.  

#### **3.5. `utils/`**  
- **Importancia**: Proporciona funciones auxiliares, como validaci√≥n de URLs (`helpers.py`) y configuraci√≥n de logging (`logger.py`).  
- **Ventajas**: Promueve la reutilizaci√≥n de c√≥digo y reduce la duplicaci√≥n. Facilita la depuraci√≥n y el monitoreo del proyecto.  

#### **3.6. `db/`**  
- **Importancia**: Contiene los modelos de base de datos (`models.py`) y la configuraci√≥n de la conexi√≥n (`database.py`).  
- **Ventajas**: Separa la l√≥gica de acceso a datos del resto del c√≥digo, lo que facilita la migraci√≥n a otros sistemas de bases de datos.  

#### **3.7. `web/`**  
- **Importancia**: Implementa la API RESTful usando Flask (`app.py`, `routes.py`) y los archivos est√°ticos (`templates/`, `static/`).  
- **Ventajas**: Permite exponer los datos scrapeados a trav√©s de una interfaz web, lo que facilita la integraci√≥n con otros sistemas.  

---

### **4. Directorio `tests/`**  
- **Importancia**: Contiene pruebas automatizadas para cada m√≥dulo del proyecto (`test_modules.py`) y configuraciones comunes (`conftest.py`).  
- **Ventajas**: Asegura la calidad del c√≥digo y detecta errores temprano. Facilita la refactorizaci√≥n y el mantenimiento.  

---

### **5. Directorio `logs/`**  
- **Importancia**: Almacena archivos de registro (`scraping.log`) que documentan la actividad del scraper.  
- **Ventajas**: Facilita la depuraci√≥n y el monitoreo del sistema en producci√≥n.  

---

### **6. Directorio `outputs/`**  
- **Importancia**: Contiene los resultados del scraping en formato JSON o SQL.  
- **Ventajas**: Centraliza los datos extra√≠dos, lo que facilita su an√°lisis y uso posterior.  

---

### **7. Directorio `static/`**  
- **Importancia**: Almacena archivos est√°ticos (CSS, JS, im√°genes) para la interfaz web.  
- **Ventajas**: Separa el contenido est√°tico del c√≥digo din√°mico, lo que mejora el rendimiento y la organizaci√≥n.  

---

### **Ventajas Generales de la Estructura**
1. **Modularidad**: Cada componente tiene una responsabilidad clara, lo que facilita la escalabilidad y el mantenimiento.  
2. **Reutilizaci√≥n de C√≥digo**: Funciones comunes (por ejemplo, logging, manejo de datos) est√°n centralizadas en m√≥dulos reutilizables.  
3. **Claridad y Organizaci√≥n**: La estructura jer√°rquica y los nombres descriptivos hacen que el proyecto sea f√°cil de entender y navegar.  
4. **Escalabilidad**: Nuevos m√≥dulos (por ejemplo, extractores para otros sitios) pueden agregarse sin afectar el c√≥digo existente.  
5. **Colaboraci√≥n**: Facilita el trabajo en equipo al separar responsabilidades y proporcionar una estructura clara.  
6. **Pruebas y Depuraci√≥n**: Las pruebas automatizadas y los logs mejoran la calidad del c√≥digo y simplifican la detecci√≥n de errores.  

# üìà Diagrama de Clases 

```mermaid

classDiagram
    class WebDataExtractor {
        <<Abstract>>
        - _url: str
        - _html_content: str
        - _data: List[Dict]
        + logger: Logger
        + download()*: str
        + parse()*: List[Dict]
        + store()*: bool
        + scrape(): List[Dict]
        + iter_data(): Generator
        + url: str
        + html_content: str
        + data: List[Dict]
    }

    class StaticPageExtractor {
        - _selectores: Dict
        + get_selectores(): Dict
        + get_cache_filename(): str
        + download(): str
        + parse(): List[Dict]
        + store(): bool
        + selectores: Dict
    }

    class DynamicPageExtractor {
        - _tienda: str
        - _num_productos: int
        + driver: WebDriver
        + detectar_tienda(): str
        + configurar_driver(): WebDriver
        + download(): str
        + parse()*: List[Dict]
        + store(): bool
        + tienda: str
        + num_productos: int
    }

    class EcommerceExtractor {
        + tienda: str
        + parse(): List[Dict]
        + store(): bool
        + extraer_texto(): str
        + extraer_imagen(): str
        + extraer_precio(): str
        + procesar_descuento(): str
        + extraer_puntuacion(): Dict
        + extraer_url(): str
        + extraer_descripcion(): str
    }

    class RealEstateExtractor {
        + tienda: str
        + ubicacion: str
        + user_agent: str
        + extraer_ubicacion_url(): str
        + configurar_driver(): WebDriver
        + download(): str
        + parse(): List[Dict]
        + store(): bool
        + manejar_popups(): None
        + configurar_tipo_negocio(): None
        + configurar_ubicacion_exacta(): None
        + ejecutar_busqueda(): None
        + extraer_titulo(): str
        + extraer_precio(): str
        + formatear_precio(): str
        + extraer_cardblock(): str
        + extraer_url(): str
    }

    class DataHandler {
        - _data: Union[Dict, List[Dict]]
        - _storage_format: str
        - _logger: Logger
        + store_data(): bool
        + store_json(): bool
        + store_sql(): bool
        + generate_report(): str
        + categorize_data(): Dict
        + data: Union[Dict, List[Dict]]
        + storage_format: str
        + logger: Logger
    }

    class ScrapingCoordinator {
        + tasks: List[Dict]
        + max_workers: int
        + results: List[Dict]
        + validate_tasks(): None
        + select_extractor(): WebDataExtractor
        + process_task(): Dict
        + run(): Dict
    }

    class ScrapedData {
        + id: int
        + url: str
        + tipo: str
        + contenido: str
        + fecha_extraccion: DateTime
    }

    class App {
        + app: Flask
        + create_app(): Flask
        + index(): str
        + not_found(): Dict
        + server_error(): Dict
    }

    class ProductData {
        + title: str
        + image: str
        + price_original: str
        + price_sell: str
        + discount: str
        + rating: Dict
        + url: str
        + description: str
        + to_dict(): Dict
    }

    class Helpers {
        + validate_url(): bool
        + create_directory_structure(): None
        + clean_filename(): str
        + calculate_stats(): Dict
        + generate_hash(): str
    }

    class Logger {
        + setup_logger(): Logger
        + get_logger(): Logger
    }

    WebDataExtractor <|-- StaticPageExtractor
    WebDataExtractor <|-- DynamicPageExtractor
    DynamicPageExtractor <|-- EcommerceExtractor
    DynamicPageExtractor <|-- RealEstateExtractor

    EcommerceExtractor --> ProductData
    EcommerceExtractor --> DataHandler
    RealEstateExtractor --> DataHandler
    ScrapingCoordinator --> WebDataExtractor
    ScrapingCoordinator --> StaticPageExtractor
    ScrapingCoordinator --> EcommerceExtractor
    ScrapingCoordinator --> RealEstateExtractor
    DataHandler --> ScrapedData
    App --> ScrapingCoordinator
    App --> DataHandler

    Logger --> DynamicPageExtractor
    Logger --> RealEstateExtractor
    Logger --> DataHandler
    Logger --> ScrapingCoordinator

    Helpers --> EcommerceExtractor
    Helpers --> RealEstateExtractor
```

# ‚õÖ Relaciones en el Diagrama de Clases 

## 1. Herencia (Relaci√≥n "es un")  
### Descripci√≥n  
Indica que una clase es una especializaci√≥n de otra. La clase hija hereda atributos y m√©todos de la clase padre.  

### Relaciones  
- **WebDataExtractor ‚Üí StaticPageExtractor:**  
  - `StaticPageExtractor` es una especializaci√≥n de `WebDataExtractor` para manejar sitios est√°ticos (HTML/CSS).  
  - Hereda m√©todos como `download()`, `parse()` y `store()`, pero los implementa de manera espec√≠fica para p√°ginas est√°ticas.  
 
- **WebDataExtractor ‚Üí DynamicPageExtractor:**  
  - `DynamicPageExtractor` es una especializaci√≥n de `WebDataExtractor` para manejar sitios din√°micos (JavaScript).  
  - Hereda m√©todos como `download()`, `parse()` y `store()`, pero los implementa usando Selenium para interactuar con contenido din√°mico.  

- **DynamicPageExtractor ‚Üí EcommerceExtractor:**  
  - `EcommerceExtractor` es una especializaci√≥n de `DynamicPageExtractor` para manejar sitios de e-commerce (por ejemplo, MercadoLibre, Alkosto).  
  - Implementa m√©todos espec√≠ficos para extraer datos de productos, como precios, im√°genes y descripciones.  

- **DynamicPageExtractor ‚Üí RealEstateExtractor:**  
  - `RealEstateExtractor` es una especializaci√≥n de `DynamicPageExtractor` para manejar sitios de bienes ra√≠ces (por ejemplo, Metrocuadrado).  
  - Implementa m√©todos espec√≠ficos para extraer datos de propiedades, como precios, √°reas y ubicaciones.  

---

## 2. Composici√≥n (Relaci√≥n "tiene un")  
### Descripci√≥n  
Indica que una clase est√° compuesta por otras clases. La clase contenedora depende de las clases que la componen.  

### Relaciones  
- **EcommerceExtractor ‚Üí ProductData:**  
  - `EcommerceExtractor` utiliza `ProductData` para representar los datos de un producto (por ejemplo, t√≠tulo, precio, imagen).  
  - `ProductData` es una clase auxiliar que encapsula la estructura de los datos de un producto.  

- **EcommerceExtractor ‚Üí DataHandler:**  
  - `EcommerceExtractor` utiliza `DataHandler` para almacenar los datos extra√≠dos en JSON o SQL.  

- **RealEstateExtractor ‚Üí DataHandler:**  
  - `RealEstateExtractor` utiliza `DataHandler` para almacenar los datos extra√≠dos en JSON o SQL.  

- **ScrapingCoordinator ‚Üí StaticPageExtractor, EcommerceExtractor, RealEstateExtractor:**  
  - `ScrapingCoordinator` utiliza estas clases para ejecutar tareas de scraping.  
  - Coordina la ejecuci√≥n de m√∫ltiples tareas, seleccionando el extractor adecuado para cada URL.  

---

## 3. Asociaci√≥n (Relaci√≥n "usa")  
### Descripci√≥n  
Indica que una clase utiliza otra clase para realizar una tarea espec√≠fica, pero no hay una dependencia fuerte entre ellas.  

### Relaciones  
- **DataHandler ‚Üí ScrapedData:**  
  - `DataHandler` utiliza `ScrapedData` para almacenar los datos extra√≠dos en la base de datos.  
  - `ScrapedData` es un modelo de base de datos que representa los datos scrapeados.  

- **App ‚Üí ScrapingCoordinator:**  
  - `App` utiliza `ScrapingCoordinator` para gestionar las tareas de scraping.  
  - `ScrapingCoordinator` es responsable de ejecutar las tareas y devolver los resultados.  

- **App ‚Üí DataHandler:**  
  - `App` utiliza `DataHandler` para acceder a los datos almacenados y mostrarlos a trav√©s de la API.  

---

## 4. Dependencia de Utilidades (Relaci√≥n "usa")  
### Descripci√≥n  
Indica que una clase depende de una clase de utilidad para realizar tareas espec√≠ficas.  

### Relaciones  
- **DynamicPageExtractor, RealEstateExtractor, DataHandler, ScrapingCoordinator ‚Üí Logger:**  
  - Estas clases utilizan `Logger` para registrar eventos, errores y actividades durante la ejecuci√≥n del scraping.  
  - `Logger` es una clase de utilidad que centraliza la configuraci√≥n de logging.  

- **EcommerceExtractor, RealEstateExtractor ‚Üí Helpers:**  
  - Estas clases utilizan `Helpers` para realizar tareas comunes, como validar URLs, limpiar nombres de archivos y generar hashes.  
  - `Helpers` es una clase de utilidad que proporciona funciones auxiliares.  

---

## 5. Relaci√≥n entre ScrapedData y DataHandler  
### Descripci√≥n  
`DataHandler` utiliza `ScrapedData` para almacenar los datos extra√≠dos en la base de datos.  

### C√≥mo se afectan  
- `DataHandler` toma los datos extra√≠dos por los extractores (`StaticPageExtractor`, `EcommerceExtractor`, `RealEstateExtractor`) y los convierte en instancias de `ScrapedData`.  
- `ScrapedData` es un modelo de base de datos que define la estructura de los datos almacenados (por ejemplo, URL, tipo de datos, contenido, fecha de extracci√≥n).  

---

## 6. Relaci√≥n entre App y ScrapingCoordinator/DataHandler  
### Descripci√≥n  
`App` utiliza `ScrapingCoordinator` y `DataHandler` para gestionar la API y las tareas de scraping.  

### C√≥mo se afectan  
- `App` expone una API RESTful que permite a los usuarios iniciar tareas de scraping y consultar los datos almacenados.  
- `ScrapingCoordinator` ejecuta las tareas de scraping y devuelve los resultados a `App`.  
- `DataHandler` proporciona acceso a los datos almacenados, que `App` devuelve como respuestas de la API.  

---

# üíé **Utilidades**

## M√≥dulo `Logger`
El m√≥dulo `Logger` gestiona el registro de eventos en la aplicaci√≥n mediante el m√≥dulo `logging` de Python, permitiendo un rastreo flexible. Se integra con `os` para la gesti√≥n de directorios y `typing` para mejorar la claridad del c√≥digo.

### 1. Configuraci√≥n del Logger (`setup_logger`)
- **Prop√≥sito**: Configura el logger con handlers para archivo y consola.
- **Funcionalidades**:
  - Define el nivel de logging (`INFO`, `DEBUG`).
  - Elimina handlers duplicados.
  - Crea el directorio de logs si no existe.
  - Establece un formato est√°ndar para los logs.
  - Configura un handler de archivo rotativo (10 MB, 5 respaldos).
  - Agrega un handler de consola opcional.
  - Suprime logs de Selenium para evitar ruido.

```python
def setup_logger(config: Dict[str, Any]) -> logging.Logger:
    """Configura el logger ra√≠z con handlers para archivo y consola"""
    logger = logging.getLogger()
    logger.setLevel(config.get('level', 'INFO').upper())

    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    os.makedirs(config.get('log_dir', 'logs'), exist_ok=True)

    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    file_handler = RotatingFileHandler(
        filename=os.path.join(config.get('log_dir', 'logs'), 'scraping.log'),
        maxBytes=10*1024*1024,  
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    if config.get('enable_console', True):
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
    logging.getLogger("selenium").setLevel(logging.WARNING)

    return logger
```
## 2. Obtenci√≥n del Logger (`get_logger`)

### Prop√≥sito
Retorna un logger configurado o uno b√°sico si `setup_logger` no se ha ejecutado.

### Funcionalidades
- Crea un logger b√°sico con handler de consola si no hay configuraciones previas.
- Establece `INFO` como nivel de logging predeterminado.

```python
def get_logger(name: str = None) -> logging.Logger:
    """Obtiene un logger configurado o un logger b√°sico si setup_logger no se ejecut√≥."""
    logger = logging.getLogger(name or "ScrapingLogger")

    if not logger.hasHandlers():
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)

    return logger
```

# **M√≥dulo `Helpers`**  
El m√≥dulo `Helpers` proporciona funciones auxiliares esenciales para el proyecto, incluyendo validaci√≥n de URLs, gesti√≥n de directorios, limpieza de nombres de archivos, c√°lculo de estad√≠sticas y generaci√≥n de hashes.  

## **L√≥gica del C√≥digo**  

### **1. Validaci√≥n de URLs (`validate_url`)**
- **Prop√≥sito**: Verifica si una URL tiene un formato v√°lido.
- **Funcionalidad**: Usa una expresi√≥n regular para validar el protocolo, dominio, puerto opcional y ruta opcional.

```python
def validate_url(url: str) -> bool:
    """Valida que una URL tenga formato correcto"""
    regex = re.compile(r'^(https?://)?(([A-Z0-9-]+\.)+[A-Z]{2,63})(:\d+)?(/.*)?$', re.IGNORECASE)
    return re.match(regex, url) is not None
```
## 2. Creaci√≥n de Directorios (`create_directory_structure`)

**Prop√≥sito**: Crea la estructura de directorios para almacenar los resultados del scraping.  
**Funcionalidad**: Usa `os.makedirs` para crear los directorios si no existen.  

```python
def create_directory_structure(base_path: str = "outputs") -> None:
    """Crea la estructura de directorios necesaria"""
    directories = [
        base_path,
        os.path.join(base_path, "static_pages_extractors"),
        os.path.join(base_path, "dynamic_extractors", "e-commerce"),
        os.path.join(base_path, "dynamic_extractors", "real_state"),
        "logs"
    ]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
```
## 3. Limpieza de Nombres de Archivos (`clean_filename`)

**Prop√≥sito**: Asegura que los nombres de archivos sean v√°lidos en todos los sistemas.  
**Funcionalidad**: Elimina caracteres especiales y espacios innecesarios.  

```python
def clean_filename(filename: str, max_length: int = 100) -> str:
    """Limpia un nombre de archivo para hacerlo v√°lido en todos los sistemas"""
    cleaned = re.sub(r'[\\/*?:"<>|]', "_", filename.strip())
    cleaned = re.sub(r'_{2,}', '_', cleaned)
    return cleaned[:max_length].strip('_')
```
## 4. C√°lculo de Estad√≠sticas (`calculate_stats`)

**Prop√≥sito**: Obtiene m√©tricas sobre los resultados del scraping.  
**Funcionalidad**:  
- Cuenta √©xitos y errores.  
- Calcula tasas de √©xito y error.  
- Agrupa errores por tipo.  

```python
def calculate_stats(results: List[Dict]) -> Dict[str, Union[int, float]]:
    """Calcula estad√≠sticas de los resultados del scraping"""
    stats = {'total': len(results), 'success': 0, 'errors': 0, 'error_types': {}, 'avg_time': 0.0}
    
    for result in results:
        if 'error' in result:
            stats['errors'] += 1
            error_type = result['error'].split(':')[0]
            stats['error_types'][error_type] = stats['error_types'].get(error_type, 0) + 1
        else:
            stats['success'] += 1

    if stats['total'] > 0:
        stats['success_rate'] = (stats['success'] / stats['total']) * 100
        stats['error_rate'] = (stats['errors'] / stats['total']) * 100
    
    return stats
```
## 5. Generaci√≥n de Hash (`generate_hash`)

**Prop√≥sito**: Crea un identificador √∫nico para un contenido.  
**Funcionalidad**: Usa el algoritmo MD5 y retorna los primeros `length` caracteres del hash.  

```python
def generate_hash(content: str, length: int = 8) -> str:
    """Genera un hash √∫nico para contenido"""
    return hashlib.md5(content.encode()).hexdigest()[:length]
```
# üìú **Clase Base**

## **WebDataExtractor**

La clase `WebDataExtractor` es una **clase abstracta** que define la interfaz y el flujo base para la extracci√≥n de datos de p√°ginas web. Establece un proceso est√°ndar de scraping que incluye:

1. Descargar el contenido HTML.
2. Parsear la informaci√≥n relevante.
3. Almacenar los datos extra√≠dos.

Las clases hijas (por ejemplo, `StaticPageExtractor` y `DynamicPageExtractor`) implementan los m√©todos abstractos definidos aqu√≠, pero siempre respetando este mismo flujo.

---

### Rol dentro del sistema y flujo de datos

`WebDataExtractor` es el **punto de partida del pipeline de datos** del proyecto. Su objetivo principal es garantizar que todos los extractores produzcan informaci√≥n **consistente** y f√°cil de reutilizar en el resto del sistema.

- Define un **contrato com√∫n** (`download()`, `parse()`, `store()`, `scrape()`) para cualquier extractor.
- Asegura que el resultado final sea una **lista de diccionarios** con una estructura homog√©nea, que:
  - `DataHandler` puede guardar en JSON o SQLite.
  - El m√≥dulo Java puede transformar en objetos `Producto` para cargarlos en estructuras de datos (`ArrayList`, `AVLTree`, `BST`, `Heap`).

Gracias a esto, es posible a√±adir nuevas fuentes de datos sin modificar la l√≥gica de almacenamiento ni la l√≥gica de consulta.

---

## **M√©todos clave**

### 1. M√©todos abstractos

#### `download(self)`
- Descarga el contenido HTML de la URL.
- La implementaci√≥n concreta depende de la subclase (por ejemplo, `requests` para p√°ginas est√°ticas o `Selenium` para p√°ginas din√°micas).

#### `parse(self)`
- Parsea el HTML descargado y extrae la informaci√≥n deseada.
- Debe devolver **una lista de diccionarios** con una estructura consistente  
  (por ejemplo, `[{ "title": ..., "price": ..., "url": ... }, ...]`).

#### `store(self)`
- Almacena los datos extra√≠dos en el formato configurado (JSON, base de datos SQLite, etc., normalmente a trav√©s de `DataHandler`).
- Devuelve `True` si el almacenamiento fue exitoso y `False` en caso contrario.

---

### 2. M√©todos concretos

#### `scrape(self)`
- Orquesta el proceso completo de scraping:
  1. Llama a `download()` para obtener el HTML.
  2. Llama a `parse()` para construir la lista de diccionarios.
  3. Llama a `store()` para persistir los datos.
- Devuelve la lista de datos extra√≠dos o `None` si ocurre un error.

#### `iter_data(self)`
- Generador que permite iterar sobre los datos extra√≠dos.
- √ötil para manejar grandes vol√∫menes de informaci√≥n sin cargar toda la lista en memoria de una sola vez.

---

## **Atributos clave**

- **`_url`**  
  URL objetivo a scrapear (privado). Se accede mediante la propiedad `url`.

- **`_html_content`**  
  Contenido HTML descargado (privado). Se accede mediante la propiedad `html_content`.

- **`_data`**  
  Lista de diccionarios con los datos extra√≠dos (privado). Se accede mediante la propiedad `data`.

- **`logger`**  
  Manejador de logging utilizado para registrar eventos, advertencias y errores durante el proceso de scraping.

---

## **Ejemplo de uso**

```python
class StaticPageExtractor(WebDataExtractor):
    def download(self):
        # Implementaci√≥n espec√≠fica para p√°ginas est√°ticas
        pass

    def parse(self):
        # Implementaci√≥n espec√≠fica para p√°ginas est√°ticas
        pass

    def store(self):
        # Implementaci√≥n espec√≠fica para p√°ginas est√°ticas
        pass

extractor = StaticPageExtractor("https://example.com")
data = extractor.scrape()  # Ejecuta el proceso completo de scraping
```
# üõ∏ **Extractores**
## **`static_page_extractor.py`**

Implementa un extractor para p√°ginas web **est√°ticas**, heredando de la clase abstracta `WebDataExtractor`.  
Este m√≥dulo se encarga de:

- **Descargar el contenido HTML** de p√°ginas est√°ticas usando `requests`, con gesti√≥n de cach√© y reintentos (ver m√©todo `download()`).
- **Parsear el HTML** para extraer informaci√≥n estructurada (t√≠tulo, infobox, contenido, im√°genes, listas y tablas) usando `BeautifulSoup` (ver m√©todo `parse()`).
- **Entregar los datos ya estructurados** como diccionarios, listos para ser almacenados por `DataHandler` en JSON y/o base de datos SQLite (ver m√©todo `store()`).
- **Personalizar selectores** y par√°metros a trav√©s de un archivo de configuraci√≥n (ver atributo `_selectores` y m√©todo `get_selectores()`).

### üîÅ Rol dentro del sistema y relaci√≥n con los datos

- Es la **fuente principal de datos estructurados** para p√°ginas est√°ticas (por ejemplo, Wikipedia y Fandom).
- Produce una **lista de diccionarios** con campos consistentes (por ejemplo, `title`, `infobox`, `content`), lo que facilita:
  - que `DataHandler` los guarde en JSON/SQLite sin conocer los detalles de cada sitio,
  - que estos registros puedan transformarse despu√©s en objetos de m√°s alto nivel (por ejemplo, productos o entradas) y, eventualmente, cargarse en estructuras de datos en otros m√≥dulos.
- La l√≥gica de cach√© evita descargas repetidas, reduciendo el tiempo de ejecuci√≥n y el n√∫mero de peticiones a los servidores de origen.

---

## **Dependencias**

- **M√≥dulos principales**:
  - `hashlib`: para generar hashes √∫nicos (ver m√©todo `get_cache_filename()`).
  - `json`: para manejar datos en formato JSON (ver m√©todo `store()`).
  - `os`: para interactuar con el sistema de archivos (ver m√©todo `get_cache_filename()`).
  - `requests`: para realizar solicitudes HTTP (ver m√©todo `download()`).
  - `time`: para manejar tiempos de espera y reintentos (ver m√©todo `download()`).
  - `BeautifulSoup`: para parsear HTML (ver m√©todo `parse()`).
  - `urllib.parse.urljoin`: para construir URLs absolutas (ver m√©todo `parse()`).

---

## **Fragmentos de C√≥digo Destacados**

### **1. Descarga con Cach√©**
```python
def download(self):
    cache_file = self.get_cache_filename()
    if self.url in CACHE:
        return CACHE[self.url]
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            return f.read()
    # L√≥gica de descarga y almacenamiento en cach√©...
```
### **2. Descarga con Cach√©** 
```python def parse(self):
    soup = BeautifulSoup(self.html_content, 'html.parser')
    title = soup.find("h1").get_text(strip=True)  # Extrae el t√≠tulo
    infobox = self._extraer_infobox(soup)  # Extrae la infobox
    content = self._extraer_contenido(soup)  # Extrae el contenido
    return {"title": title, "infobox": infobox, "content": content}
 ```
### **3. Descarga con Cach√©** 
```python def store(self):
    handler = DataHandler(self.data, storage_format='both', logger=self.logger)
    return handler.store_data(url=self.url, tipo="static")
```
## **M√≥dulo: `dynamic_page_extractor.py`**

Implementa un extractor base para p√°ginas web **din√°micas**, heredando de la clase abstracta `WebDataExtractor`.  
Este m√≥dulo se encarga de:

- **Cargar p√°ginas din√°micas** usando `Selenium WebDriver` en modo headless (ver m√©todo `download()`).
- **Esperar a que se renderice el contenido** din√°mico antes de capturar el HTML (uso de `WebDriverWait` en `download()`).
- **Entregar el HTML resultante** para que las subclases (como `EcommerceExtractor` y `RealEstateExtractor`) lo procesen con `BeautifulSoup` (`parse()` en subclases).
- **Almacenar los datos extra√≠dos** mediante `DataHandler` en JSON o en una base de datos SQLite (ver m√©todo `save_store()` / `store()`).

---

### üîÅ Rol dentro del sistema y relaci√≥n con los datos

- Proporciona una **capa com√∫n para todas las p√°ginas que requieren JavaScript** para mostrar su contenido (e-commerce, bienes ra√≠ces, etc.).
- Separa la l√≥gica de:
  - configuraci√≥n y apertura del navegador,
  - espera a que los elementos se carguen,
  - obtenci√≥n del HTML din√°mico,
de la l√≥gica espec√≠fica de parseo que implementan las subclases.
- Garantiza que, una vez parseado el HTML por las subclases, el resultado se convierta en **listas de diccionarios** coherentes, listas para:
  - ser almacenadas por `DataHandler` en JSON/SQLite,
  - ser utilizadas posteriormente por otros m√≥dulos (por ejemplo, transformadas a objetos `Producto` en Java y cargadas en estructuras de datos).

---

### **Dependencias**

- **`selenium`**: para automatizar la interacci√≥n con navegadores web (ver m√©todo `download()`).
- **`BeautifulSoup`**: para parsear el HTML resultante (en las subclases que implementan `parse()`).
- **`random`**: para seleccionar un agente de usuario aleatorio (inicializaci√≥n de `USER_AGENT_DINAMICOS`).
- **`time`**: para manejar tiempos de espera y pausas en la carga (ver m√©todo `download()`).
- **`urllib.parse`**: para trabajar con URLs y detectar la tienda (ver m√©todo `detectar_tienda()`).

---

### **Clase principal: `DynamicPageExtractor`**

#### **Atributos**

- **`_tienda`**: tipo de tienda detectada (por ejemplo `"mercadolibre"`, `"alkosto"`) (ver m√©todo `detectar_tienda()`).
- **`_num_productos`**: n√∫mero de productos a extraer (ver propiedad `num_productos`).
- **`driver`**: instancia de `Selenium WebDriver` utilizada para cargar la p√°gina (ver m√©todo `configurar_driver()`).

#### **M√©todos**

- **`download()`**: descarga el contenido din√°mico usando Selenium (abre la p√°gina, espera a que cargue el DOM y devuelve el HTML).
- **`parse()`**: m√©todo abstracto; las subclases lo implementan para extraer datos espec√≠ficos del HTML.
- **`save_store()` / `store()`**: delegan el almacenamiento de los datos extra√≠dos en `DataHandler`, permitiendo guardar en JSON y/o SQLite con el tipo `"dynamic"`.
- **`detectar_tienda()`**: detecta la tienda basada en el dominio de la URL para adaptar la configuraci√≥n.
- **`configurar_driver()`**: configura el WebDriver (modo headless, user-agent, etc.) antes de realizar la descarga.

---

### **Ejemplo de uso**

```python
extractor = DynamicPageExtractor("https://www.mercadolibre.com.co")
extractor.download()        # Descarga el contenido din√°mico
data = extractor.parse()    # Parsea el HTML (implementado en subclases)
extractor.save_store()      # Almacena los datos
```
### **Descarga con Selenium**
```python 
def download(self):
    opciones = Options()
    opciones.add_argument("--headless=new")  # Modo headless
    opciones.add_argument(f"user-agent={random.choice(USER_AGENT_DINAMICOS)}")
    driver = webdriver.Chrome(options=opciones)
    driver.get(self.url)
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    return driver.page_source
```

### **Detecci√≥n de Tienda**
```python
def detectar_tienda(self):
    dominio = urlparse(self.url).netloc.lower()
    for tienda, dominios in dominios_tiendas.items():
        if any(subdominio in dominio for subdominio in dominios):
            return tienda
    raise ValueError(f"No se reconoce la tienda para el dominio: {dominio}")
```
### **Almacenamiento de Datos**
```python   def save_store(self):
    handler = DataHandler(self.data, storage_format='both', logger=self.logger)
    return handler.store_data(url=self.url, tipo="dynamic")
```
## **M√≥dulo: `ecommerce_extractor.py`**

Implementa un extractor de datos para p√°ginas web din√°micas de **e-commerce**, como MercadoLibre y Alkosto. Utiliza `Selenium` para renderizar el contenido din√°mico y `BeautifulSoup` para parsear el HTML. Este m√≥dulo permite extraer informaci√≥n estructurada de productos, como t√≠tulos, precios, im√°genes, descuentos y descripciones.

### **Caracter√≠sticas principales**

- **Paginaci√≥n autom√°tica**: navega por m√∫ltiples p√°ginas de resultados.
- **Manejo de errores robusto**: reintentos y logging detallado.
- **Almacenamiento flexible**: guarda datos en JSON o en base de datos SQLite (a trav√©s de `DataHandler`).

---

### üîÅ Rol dentro del sistema y relaci√≥n con los datos

- Especializa el comportamiento de `DynamicPageExtractor` para sitios de **e-commerce**, transformando el HTML din√°mico en registros de productos con una estructura uniforme.
- Utiliza la clase `ProductData` para organizar los campos de cada producto (t√≠tulo, precio, imagen, descuento, rating, URL, descripci√≥n) y luego convertirlos a **diccionarios** mediante `to_dict()`.
- Genera una **lista de diccionarios** que:
  - `DataHandler` puede almacenar en JSON y/o SQLite sin conocer detalles de cada tienda.
  - puede ser consumida posteriormente por otros m√≥dulos (por ejemplo, el m√≥dulo Java) para crear objetos `Producto` y cargarlos en estructuras de datos como `ArrayList`, `AVLTree`, `BST` o `Heap`.
- Centraliza la l√≥gica de extracci√≥n espec√≠fica de e-commerce, de modo que el resto del sistema solo trabaje con datos ya limpios y estructurados.

---

### **Dependencias**

- **`selenium`**: para interactuar con p√°ginas din√°micas (ver m√©todo `download()` heredado de `DynamicPageExtractor`).
- **`BeautifulSoup`**: para parsear HTML (ver m√©todo `parse()`).
- **`re`**: para expresiones regulares (por ejemplo, en `extraer_puntuacion()` o limpieza de precios).
- **`urllib.parse`**: para manejar URLs (ver m√©todo `extraer_url()`).
- **`typing`**: para definir tipos de datos (ver atributos de `ProductData`).

---

### **Clases principales**

#### **1. `ProductData`**

Representa la informaci√≥n de un producto individual.

- **Atributos**:
  - `title`: t√≠tulo del producto.
  - `image`: URL de la imagen.
  - `price_original`: precio original.
  - `price_sell`: precio de venta.
  - `discount`: descuento aplicado.
  - `rating`: calificaci√≥n y rese√±as.
  - `url`: URL del producto.
  - `description`: descripci√≥n del producto.
- **M√©todos**:
  - `to_dict()`: convierte los datos a un diccionario est√°ndar, listo para ser almacenado por `DataHandler` o procesado por otros m√≥dulos.

#### **2. `EcommerceExtractor`**

Clase encargada de recorrer el HTML de resultados de e-commerce y construir los registros de productos.

- **Atributos**:
  - `tienda`: tipo de tienda (e.g., `"mercadolibre"`, `"alkosto"`).
  - `num_productos`: n√∫mero de productos a extraer.
- **M√©todos**:
  - `parse()`: extrae y estructura los datos de productos (crea instancias de `ProductData` y las convierte a diccionarios).
  - `extraer_texto()`: extrae texto de un elemento HTML.
  - `extraer_imagen()`: extrae la URL de la imagen del producto.
  - `extraer_precio()`: extrae y formatea el precio.
  - `procesar_descuento()`: extrae el porcentaje de descuento.
  - `extraer_puntuacion()`: extrae la calificaci√≥n del producto.
  - `extraer_url()`: construye la URL absoluta del producto.
  - `extraer_descripcion()`: extrae la descripci√≥n del producto.
  - `store()`: utiliza `DataHandler` para almacenar los datos con el tipo `"e-commerce"`.

---

### **Ejemplo de uso**

```python
extractor = EcommerceExtractor(
    "https://www.mercadolibre.com.co",
    tienda="mercadolibre",
    num_productos=5
)
extractor.download()   # Descarga el contenido din√°mico
data = extractor.parse()   # Parsea los datos de productos
extractor.store()      # Almacena los datos en JSON/SQL
```
## **Extracci√≥n de Precio**
```python
def extraer_precio(self, elemento: Tag, selector_padre: Dict) -> str:
    if not selector_padre:
        return "Precio no disponible"
    elemento_padre = elemento.find(selector_padre["tag"], class_=selector_padre.get("class"))
    if not elemento_padre:
        return "Precio no encontrado"
    return elemento_padre.get_text(strip=True)
```
## **Extracci√≥n de Imagen**
```python
def extraer_imagen(self, elemento: Tag, selector: Dict) -> Union[str, None]:
    contenedor = elemento.find(selector["tag"], class_=selector.get("class"))
    if not contenedor:
        return None
    img_element = contenedor.find("img")
    return img_element["src"] if img_element else None
```
## **Almacenamiento de Datos**
```python
def store(self) -> bool:
    handler = DataHandler(self.data, storage_format='both', logger=self.logger)
    return handler.store_data(url=self.url, tipo="e-commerce")
```
---

### **Caracter√≠sticas principales**

- **Paginaci√≥n autom√°tica**: navega por m√∫ltiples p√°ginas de resultados hasta alcanzar el n√∫mero de propiedades solicitado.
- **Manejo de errores robusto**: reintentos y logging detallado para registrar fallos de carga o parseo.
- **Almacenamiento flexible**: guarda los datos en JSON o en base de datos SQLite a trav√©s de `DataHandler`.

---

### üîÅ Rol dentro del sistema y relaci√≥n con los datos

- Especializa el comportamiento de `DynamicPageExtractor` para portales de **bienes ra√≠ces**, adaptando selectores y l√≥gica de navegaci√≥n a este tipo de sitios.
- Transforma el HTML din√°mico en una **lista de diccionarios** con campos consistentes (por ejemplo, `title`, `price`, `area`, `rooms`, `bathrooms`, `url`), que:
  - `DataHandler` puede almacenar en JSON y/o SQLite (`tipo="real_state"`).
  - pueden ser utilizados posteriormente por otros m√≥dulos para an√°lisis o, si se desea, convertidos en objetos equivalentes a `Producto` y cargados en estructuras de datos (√°rboles, listas, etc.).
- Centraliza la l√≥gica espec√≠fica de extracci√≥n en portales inmobiliarios, de modo que el resto del sistema no necesita conocer c√≥mo interactuar con cada p√°gina, solo consumir los datos ya estructurados.

---

### **Dependencias**

- **M√≥dulos principales**:
  - `selenium`: para interactuar con p√°ginas din√°micas y controlar el navegador (ver m√©todo `download()`).
  - `BeautifulSoup`: para parsear HTML (ver m√©todo `parse()`).
  - `re`: para trabajar con expresiones regulares (por ejemplo, en `extraer_precio()` o limpieza de datos).
  - `urllib.parse`: para manejar URLs (ver m√©todo `extraer_url()`).
  - `typing`: para definir tipos de datos (anotaciones en m√©todos y atributos).
  - `logging`: para registrar eventos y errores durante la ejecuci√≥n del scraper.

---

### **Clase principal: `RealEstateExtractor`**

- **Atributos**:
  - `tienda`: portal inmobiliario (por ejemplo `"metrocuadrado"`).
  - `num_productos`: n√∫mero de propiedades a extraer.
  - `driver`: instancia de `Selenium WebDriver`.
  - Otros par√°metros relacionados con filtros de b√∫squeda (ciudad, localidad, tipo de inmueble, etc.), seg√∫n configuraci√≥n.

- **M√©todos**:
  - `download()`: descarga el contenido din√°mico usando Selenium, manejando la navegaci√≥n y el tiempo de carga.
  - `parse()`: extrae y estructura datos de propiedades (t√≠tulo, precio, URL, etc.) a partir del HTML actual.
  - `store()`: utiliza `DataHandler` para almacenar los datos en JSON y/o SQLite (`tipo="real_state"`).
  - M√©todos auxiliares como:
    - `extraer_titulo()`, `extraer_precio()`, `extraer_url()`,
    - y otros m√©todos espec√≠ficos para obtener √°rea, habitaciones, ba√±os, ubicaci√≥n, etc.

---

### **Ejemplo de uso**

```python
extractor = RealEstateExtractor("https://www.metrocuadrado.com", num_productos=5)
extractor.download()         # Descarga el contenido din√°mico
data = extractor.parse()     # Parsea los datos de propiedades
extractor.store()            # Almacena en JSON/SQL
```
## **Descarga con Selenium**
```python
def download(self):
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    driver = webdriver.Chrome(options=options)
    driver.get(self.url)
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    return driver.page_source
```

## **Extracci√≥n de Precio**
```python
def extraer_precio(self, elemento: Tag, selector: Dict) -> str:
    elemento_padre = elemento.find(selector["tag"], class_=selector.get("class"))
    return elemento_padre.get_text(strip=True) if elemento_padre else "Precio no disponible"
```

## **Almacenamiento de Datos**
```python
def store(self) -> bool:
    handler = DataHandler(self.data, storage_format='both', logger=self.logger)
    return handler.store_data(url=self.url, tipo="real_state")
```
---
``` Python
def download(self):
    while len(propiedades_unicas) < self.num_productos:
        html_page = self.driver.page_source
        nuevas_props = self.parse(html_page)
        if not self.ir_pagina_siguiente(pagina_actual):
            break
        pagina_actual += 1

```
## **Extracci√≥n de Datos**
``` Python
def parse(self):
    propiedades = []
    for prop in todos_listados:
        titulo = self.extraer_titulo(prop, config['children']['title'])
        precio = self.extraer_precio(prop, config['children']['price'])
        propiedades.append({
            "title": titulo,
            "price": precio,
            "url": self.extraer_url(prop, config['children']['url'])
        })
    return propiedade
```
## **Configuraci√≥n de Ubicaci√≥n**
``` Python
def configurar_ubicacion_exacta(self):
    input_loc = self.driver.find_element(By.CSS_SELECTOR, 'input[name="location"]')
    input_loc.send_keys(self.params['localidad'])
    WebDriverWait(self.driver, 15).until(
        EC.visibility_of_element_located((By.CSS_SELECTOR, "div.react-autosuggest__suggestions-container li:first-child"))
    ).click()
```

# üìã Manejo de Datos
## **M√≥dulo: `DataHandler`**

Clase encargada del **manejo y procesamiento de los datos extra√≠dos** durante el proceso de scraping.  
Proporciona funcionalidades para:

- **Almacenar datos** en formato JSON o SQL (m√©todos `store_json()` y `store_sql()`).
- **Generar reportes** en formato TXT o HTML (m√©todo `generate_report()`).
- **Categorizar datos** extra√≠dos (m√©todo `categorize_data()`).

---

### üîÅ Rol dentro del sistema y relaci√≥n con los datos

- Act√∫a como **puente entre los extractores y el almacenamiento persistente**:
  - recibe la lista de diccionarios generada por los extractores (`StaticPageExtractor`, `EcommerceExtractor`, `RealEstateExtractor`, etc.),
  - decide c√≥mo y d√≥nde guardarla (`json`, `sql` o `both`).
- Centraliza la l√≥gica de:
  - creaci√≥n de archivos en `outputs/`,
  - escritura de JSON,
  - inserci√≥n de registros en la base de datos (`ScrapedData`).
- Los datos guardados en la base de datos pueden ser reutilizados posteriormente por otros m√≥dulos (por ejemplo, consultados desde Java para alimentar estructuras de datos como listas, √°rboles o heaps).
- La generaci√≥n de reportes permite tener una **visi√≥n resumida** de la informaci√≥n extra√≠da, √∫til para depuraci√≥n, an√°lisis o demostraciones.

---

### **Dependencias**

- **M√≥dulos principales**:
  - `json`: para convertir datos a formato JSON (ver m√©todo `store_json()`).
  - `hashlib`: para generar nombres de archivo √∫nicos mediante hashes (ver m√©todo `store_json()`).
  - `os`: para manejar directorios y rutas de archivos (ver m√©todo `store_json()` y `generate_report()`).
  - `logging`: para registrar eventos y errores (logger interno del m√≥dulo).
  - `datetime`: para nombrar reportes con marcas de tiempo (ver `generate_report()`).
  - `ScrapedData`: modelo de base de datos para almacenar los datos scrapeados (ver m√©todo `store_sql()`).
  - `SessionLocal`: factor√≠a de sesiones para conectarse a la base de datos.

---

### **Clase principal: `DataHandler`**

- **Atributos**:
  - `_data`: datos a manejar (lista de diccionarios).
  - `_storage_format`: formato de almacenamiento (`"json"`, `"sql"`, `"both"`).
  - `_logger`: instancia de logger utilizada para registrar el proceso.

- **M√©todos**:
  - `store_data(url: str, tipo: str)`: m√©todo de alto nivel que decide si llamar a `store_json()`, `store_sql()` o a ambos, seg√∫n el formato configurado.
  - `store_json(url: str, tipo: str)`: almacena los datos en archivos JSON dentro del directorio `outputs/...`.
  - `store_sql(tipo: str)`: inserta los datos en la base de datos SQL usando el modelo `ScrapedData`.
  - `generate_report(report_type='txt')`: genera un reporte en formato TXT o HTML a partir de los datos cargados.
  - `categorize_data()`: clasifica o agrupa los datos extra√≠dos seg√∫n criterios definidos (tipo, categor√≠a, rango de precios, etc.).

---

### **Ejemplo de uso**

```python
data = [{"title": "Propiedad 1", "price": "$100,000"}]

handler = DataHandler(data, storage_format='both', logger=logger)
handler.store_data(url="https://example.com", tipo="real_state")
handler.generate_report(report_type='html')
```
---

## **Fragmentos de C√≥digo Destacados**

### **1. Almacenamiento en JSON**
```python
def store_json(self, url: str, tipo: str) -> bool:
    output_dir = os.path.join("outputs", "dynamic_extractors/real_state")
    os.makedirs(output_dir, exist_ok=True)
    filename = f"data_{hashlib.md5(url.encode()).hexdigest()[:8]}.json"
    with open(os.path.join(output_dir, filename), "w", encoding="utf-8") as f:
        json.dump(self.data, f, ensure_ascii=False, indent=4)
    return True
```

---

### **2. Almacenamiento en SQL**
```python
def store_sql(self, tipo: str) -> bool:
    session = SessionLocal()
    for item in self.data:
        new_record = ScrapedData(
            url=item.get("url"),
            tipo=tipo,
            contenido=json.dumps(item, ensure_ascii=False)
        )
        session.add(new_record)
    session.commit()
    return True
```

---

### **3. Generaci√≥n de Reportes**
```python
def generate_report(self, report_type='txt'):
    filename = os.path.join("outputs/reports", f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{report_type}")
    if report_type == 'txt':
        with open(filename, "w", encoding="utf-8") as f:
            f.write("Reporte de Datos Extra√≠dos\n")
            for item in self.data:
                f.write(f"{item}\n")
    return filename
```
## **M√≥dulo: `ScrapedData` (Modelo de Base de Datos)**

Define la estructura de la tabla `scraped_data` en la base de datos utilizando SQLAlchemy.  
Esta tabla almacena los datos extra√≠dos durante el proceso de scraping, incluyendo la URL de origen, el tipo de dato, el contenido completo y la fecha de extracci√≥n.

---

### üîÅ Rol dentro del sistema y relaci√≥n con los datos

- Act√∫a como la **representaci√≥n en base de datos** de cada registro obtenido por los extractores.
- Es el destino final cuando `DataHandler` decide almacenar la informaci√≥n en formato **SQL**:
  - cada elemento de la lista de diccionarios producida por los extractores se guarda como un registro de `ScrapedData`,
  - el campo `contenido` almacena el JSON completo de ese elemento.
- Permite que los datos scrapeados puedan:
  - **persistir entre ejecuciones** del sistema,
  - ser consultados posteriormente por otros m√≥dulos (por ejemplo, para an√°lisis o para alimentar estructuras de datos en otra capa del proyecto).

---

### **Dependencias**

- **M√≥dulos principales**:
  - `sqlalchemy`: para definir el modelo de base de datos y mapearlo a la tabla (`Column`, `Integer`, `String`, `DateTime`, `Base`).
  - `datetime`: para manejar fechas y horas (campo `fecha_extraccion` con valor por defecto).

---

### **Clase principal: `ScrapedData`**

- **Atributos**:
  - `id`: identificador √∫nico de cada registro (clave primaria).
  - `url`: URL de la cual se extrajeron los datos (cadena de hasta 500 caracteres).
  - `tipo`: tipo de datos extra√≠dos (por ejemplo `"static"`, `"e-commerce"`, `"real_state"`) (cadena de hasta 50 caracteres).
  - `contenido`: datos extra√≠dos en formato JSON (cadena de texto).
  - `fecha_extraccion`: fecha y hora de la extracci√≥n (se establece autom√°ticamente al crear el registro usando `datetime.utcnow`).

---

### **Ejemplo de uso**

```python
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine

# Crear una sesi√≥n de base de datos
engine = create_engine("sqlite:///scraping_data.db")
Session = sessionmaker(bind=engine)
session = Session()

# Crear un nuevo registro
nuevo_registro = ScrapedData(
    url="https://example.com",
    tipo="static",
    contenido='{"title": "Ejemplo", "content": "Lorem ipsum"}'
)
session.add(nuevo_registro)
session.commit()
```

### Fragmentos de C√≥digo Destacados

1. **Definici√≥n del Modelo**:
   ```python
   class ScrapedData(Base):
       __tablename__ = "scraped_data"
       id = Column(Integer, primary_key=True)
       url = Column(String(500), nullable=False)
       tipo = Column(String(50), nullable=False)
       contenido = Column(String, nullable=False)
       fecha_extraccion = Column(DateTime, default=datetime.utcnow)
   ```
---
2. **Uso de `datetime.utcnow`**:
   ```python
   fecha_extraccion = Column(DateTime, default=datetime.utcnow)
   ```
   Este campo se llena autom√°ticamente con la fecha y hora actuales en formato UTC al crear un nuevo registro.

---

## Relaci√≥n con Otros M√≥dulos
- **`DataHandler`**: Utiliza `ScrapedData` para almacenar datos en la base de datos (ver m√©todo `store_sql()` en `DataHandler`).
- **`StaticPageExtractor`, `EcommerceExtractor`, `RealEstateExtractor`**: Dependen de `ScrapedData` para persistir los datos extra√≠dos.
---
# üéÆCoordinacioÃÅn
## M√≥dulo: `ScrapingCoordinator`

Clase que coordina el scraping de m√∫ltiples tareas, gestionando tanto p√°ginas est√°ticas como din√°micas (e-commerce y bienes ra√≠ces).  
Sus principales funciones son:

- **Validar tareas**: asegura que cada tarea tenga los campos necesarios (ver m√©todo `validate_tasks()`).
- **Seleccionar extractores**: elige el extractor adecuado seg√∫n el tipo y subtipo de tarea (ver m√©todo `select_extractor()`).
- **Ejecutar tareas**: procesa las tareas de manera concurrente usando `ThreadPoolExecutor` (ver m√©todo `run()`).
- **Generar estad√≠sticas**: proporciona un resumen del proceso de scraping (ver m√©todo `run()`).

---

### üîÅ Rol dentro del sistema y flujo de tareas

- Recibe una **lista de tareas** (cada una con `url`, `type`, `subtype`, par√°metros, etc.) y se encarga de orquestar su ejecuci√≥n.
- Para cada tarea:
  - valida su estructura (`validate_tasks()`),
  - selecciona el extractor apropiado (`StaticPageExtractor`, `EcommerceExtractor`, `RealEstateExtractor`) mediante `select_extractor()`,
  - ejecuta el scraping con `process_task()`, obteniendo listas de diccionarios con datos estructurados.
- Usa **concurrencia** (`ThreadPoolExecutor`) para ejecutar varias tareas en paralelo, reduciendo el tiempo total de scraping cuando hay muchas URLs.
- Centraliza los **resultados** y genera estad√≠sticas (tareas totales, √©xitos, errores, etc.), que pueden usarse:
  - para monitorear el rendimiento del sistema,
  - o para decidir qu√© datos se almacenan despu√©s mediante `DataHandler`.

---

### Dependencias

- M√≥dulos principales:
  - `concurrent.futures`: para ejecutar tareas de manera concurrente (ver m√©todo `run()`).
  - `logging`: para registrar eventos y errores (logger interno del coordinador).
  - `StaticPageExtractor`, `EcommerceExtractor`, `RealEstateExtractor`: extractores espec√≠ficos para cada tipo de tarea (ver m√©todo `select_extractor()`).

---

### Clase principal: `ScrapingCoordinator`

- **Atributos**:
  - `tasks`: lista de tareas a ejecutar (cada tarea es t√≠picamente un diccionario con al menos una `url` y un `type`).
  - `max_workers`: n√∫mero m√°ximo de hilos para ejecuci√≥n concurrente.
  - `results`: lista con los resultados devueltos por cada tarea procesada.

- **M√©todos**:
  - `validate_tasks()`: valida la estructura de las tareas y comprueba que el tipo sea v√°lido (`static` o `dynamic`, y subtipos como `"e-commerce"` o `"real_state"`).
  - `select_extractor(task)`: selecciona el extractor adecuado seg√∫n el tipo y subtipo de la tarea.
  - `process_task(task)`: ejecuta una tarea de scraping completa (crear extractor, llamar a `scrape()` o m√©todos equivalentes, manejar errores y devolver un resultado).
  - `run()`: orquesta la ejecuci√≥n de todas las tareas usando `ThreadPoolExecutor`, acumula los resultados y construye estad√≠sticas globales del proceso.

---

### Ejemplo de uso

```python
tasks = [
    {"url": "https://example.com/static", "type": "static"},
    {"url": "https://example.com/ecommerce", "type": "dynamic", "subtype": "e-commerce"},
    {"url": "https://example.com/realstate", "type": "dynamic", "subtype": "real_state"}
]

coordinator = ScrapingCoordinator(tasks, max_workers=3)
resultados = coordinator.run()

print(resultados['statistics'])  # Muestra estad√≠sticas del scraping
```
---
### Fragmentos de C√≥digo Destacados

1. **Validaci√≥n de Tareas**:
   ```python
   def validate_tasks(self, tasks):
       for task in tasks:
           if task['type'] not in ['static', 'dynamic']:
               raise ValueError(f"Tipo de tarea inv√°lido: {task['type']}")
   ```
---
2. **Selecci√≥n de Extractor**:
   ```python
   def select_extractor(self, task):
       if task['type'] == 'static':
           return StaticPageExtractor(task['url'])
       elif task['subtype'] == 'e-commerce':
           return EcommerceExtractor(task['url'])
       elif task['subtype'] == 'real_state':
           return RealEstateExtractor(task['url'])
   ```
---
3. **Ejecuci√≥n Concurrente**:
   ```python
   def run(self):
       with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
           futures = [executor.submit(self.process_task, task) for task in self.tasks]
           for future in as_completed(futures):
               self.results.append(future.result())
   ```
---
4. **Generaci√≥n de Estad√≠sticas**:
   ```python
   stats = {
       'total_tasks': len(self.tasks),
       'success': sum(1 for r in self.results if 'data' in r),
       'errors': len(self.results) - success,
       'error_rate': f"{(errors/len(self.tasks)*100):.1f}%"
   }
   ```
---
### Relaci√≥n con Otros M√≥dulos
- **`StaticPageExtractor`, `EcommerceExtractor`, `RealEstateExtractor`**: Utilizados para ejecutar tareas espec√≠ficas.
- **`DataHandler`**: Puede ser utilizado para almacenar los resultados del scraping.

# üçµ Soporte

### **Clase: `ProductData`**

Clase auxiliar que define la **estructura de datos de un producto**.  
Agrupa en un solo lugar los campos relevantes de un √≠tem de e-commerce (t√≠tulo, precios, imagen, descuento, calificaci√≥n, URL y descripci√≥n) y ofrece un m√©todo para convertir esa informaci√≥n en un diccionario listo para ser almacenado o procesado.

---

### üîÅ Rol dentro del sistema y relaci√≥n con los datos

- Sirve como una **plantilla estructurada** para los productos extra√≠dos por `EcommerceExtractor`.
- Facilita que todos los productos tengan la **misma forma** (mismas claves y tipos de datos), lo que simplifica:
  - la conversi√≥n a diccionarios mediante `to_dict()`,
  - el almacenamiento posterior por `DataHandler` en JSON o SQL,
  - el posible uso de estos datos en otros m√≥dulos (por ejemplo, para transformarlos luego en objetos `Producto` en Java y cargarlos en estructuras de datos).
- Evita manejar ‚Äúdiccionarios sueltos‚Äù sin estructura clara, centralizando en una sola clase la definici√≥n de los campos de un producto.

---

### **Atributos**

- `title`: t√≠tulo del producto (cadena de texto).
- `image`: URL de la imagen del producto (cadena de texto o `None`).
- `price_original`: precio original del producto (cadena de texto).
- `price_sell`: precio de venta del producto (cadena de texto).
- `discount`: descuento aplicado al producto (cadena de texto, valor por defecto: `"0%"`).
- `rating`: calificaci√≥n y n√∫mero de rese√±as del producto (diccionario).
- `url`: URL del producto (cadena de texto).
- `description`: descripci√≥n del producto (cadena de texto o `None`).

---

### **M√©todos**

- `to_dict()`: convierte los datos del producto en un diccionario con todas sus claves, listo para ser consumido por `DataHandler` u otros m√≥dulos.

---

### **Ejemplo de uso**

```python
producto = ProductData()
producto.title = "Producto Ejemplo"
producto.image = "https://example.com/image.jpg"
producto.price_sell = "$100"
producto.url = "https://example.com/producto"

# Convertir a diccionario
datos_producto = producto.to_dict()
print(datos_producto)
```
---
### **Fragmento de C√≥digo Destacado**

1. **Conversi√≥n a Diccionario**:
   ```python
   def to_dict(self) -> Dict:
       return self.__dict__
   ```

---

### **Relaci√≥n con Otros M√≥dulos**
- **`EcommerceExtractor`**: Utiliza `ProductData` para representar los datos de productos extra√≠dos.
- **`DataHandler`**: Puede utilizar `ProductData` para almacenar datos en JSON o SQL.
